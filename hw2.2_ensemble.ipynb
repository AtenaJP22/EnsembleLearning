{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 2.2: Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of HW2.2\n",
    "\n",
    "The aim of this assignment is to use Ensemble Learning to solve a problem. In this way, it is aimed to understand the benefits of Ensemble Learning and to teach the usage details of different ensemble approaches with the help of ScikitLearn library. \n",
    "\n",
    "The following methods will be implemented within the scope of this assignment. These are:\n",
    "- Voting Classifier (hard & soft voting) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate moon dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2.2.1 Voting classifier in Scikit-Learn\n",
    "- Train voting classifiers in Scikit-Learn, composed of at least three diverse classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Classifier Accuracy: 0.904\n",
      "Individual Classifiers:\n",
      "RandomForestClassifier Accuracy: 0.896\n",
      "SVC Accuracy: 0.856\n",
      "KNeighborsClassifier Accuracy: 0.912\n",
      "VotingClassifier Accuracy: 0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "clf1 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "clf2 = SVC(probability=True, kernel='linear', C=0.5, random_state=42)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "\n",
    "#define your hard voting classifier\n",
    "voting_clf_hard = voting_clf_hard = VotingClassifier(estimators=[('rf', clf1), ('svc', clf2), ('knn', clf3)], voting='hard')\n",
    "\n",
    "\n",
    "\n",
    "#train your voting classifier using X_train, y_train\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the hard voting classifier\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)\n",
    "print(\"Hard Voting Classifier Accuracy:\", accuracy_hard)\n",
    "\n",
    "\n",
    "#write obtained individual classifiers. Compare them with \"Voting Classifiers (hard and soft)\" for \"X_test/y_test\" data\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Individual Classifiers:\")\n",
    "for clf in (clf1, clf2, clf3, voting_clf_hard):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{clf.__class__.__name__} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2.2.2 Bagging/Pasting\n",
    "- One way to get a diverse set of classifiers is to use very different training algorithms\n",
    "- Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set\n",
    "    - When sampling is performed with replacement, this method is called ***bagging*** \n",
    "        - short for ***bootstrap aggregating***\n",
    "    - When sampling is performed without replacement, it is called pasting\n",
    "- Both bagging and pasting allow training instances to be sampled several times across multiple predictors\n",
    "- Only bagging allows training instances to be sampled several times for the same predictor\n",
    "- Predictors can all be trained in parallel, via different CPU cores or even different servers.\n",
    "- Similarly, predictions can be made in parallel.\n",
    "- This is one of the reasons why bagging and pasting are such popular methods: they scale very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy Score: 0.904\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier class \n",
    "#  \n",
    "#   (this is an example of bagging, but if you want to use pasting instead, just set bootstrap=False). \n",
    "# The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions \n",
    "#   (â€“1 tells Scikit-Learn to use all available cores):\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the base classifier (Decision Tree in this case)\n",
    "base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# Write Bagging Classifier and train it using ensemble of 500 Decision Tree classifiers, \n",
    "#  each trained on 100 training instances randomly sampled from the training set with replacement \n",
    "\n",
    "bag_clf = BaggingClassifier(base_classifier, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# bagging accuracy score\n",
    "# Using X_test dataset calculate/print Bagging Accuracy Score (using accuracy_score metric)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_bagging = bag_clf.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(\"Bagging Accuracy Score:\", accuracy_bagging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Classifier Accuracy Score: 0.856\n"
     ]
    }
   ],
   "source": [
    "# Without bagging accuracy score\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "# Train Tree Classifier\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate \"Tree Classifier\" accuracy score\n",
    "# Using X_test dataset calculate/print Tree Classifier Accuracy Score (using accuracy_score metric)\n",
    "# Tree Classifier accuracy score\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(\"Tree Classifier Accuracy Score:\", accuracy_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO 2.2.2 Out-of-Bag evaluation\n",
    "- With bagging, some instances may be sampled several times for any given predictor, \n",
    " while others may not be sampled at all. \n",
    "- By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), \n",
    " where m is the size of the training set\n",
    "- Only about 60% of the training instances are sampled on average for each predictor\n",
    "- The remaining 40% of the training instances that are not sampled are called out-of-bag (oob) instances\n",
    "- Since a predictor never sees the oob instances during training, it can be evaluated on these instances, \n",
    "without the need for a separate validation set or cross-validation\n",
    "- In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier\n",
    " to request an automatic oob evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Bag (oob) Score: 0.9253333333333333\n",
      "Out-of-Bag (oob) Bagging Accuracy Score: 0.904\n"
     ]
    }
   ],
   "source": [
    "#write your out-of-bag (oob) classifier and train it. \n",
    "oob_bag_clf = BaggingClassifier(base_classifier, n_estimators=500, max_samples=100, bootstrap=True, oob_score=True, n_jobs=-1, random_state=42)\n",
    "oob_bag_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# According to this oob evaluation print your oob score for test dataset \n",
    "print(\"Out-of-Bag (oob) Score:\", oob_bag_clf.oob_score_)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate oob bagging accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_oob_bagging = oob_bag_clf.predict(X_test)\n",
    "accuracy_oob_bagging = accuracy_score(y_test, y_pred_oob_bagging)\n",
    "print(\"Out-of-Bag (oob) Bagging Accuracy Score:\", accuracy_oob_bagging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2.2.4 Random Forests\n",
    "- Random Forest is an ensemble of Decision Trees\n",
    "- Generally trained via the bagging method typically with max_samples set to the size of the training set\n",
    "- Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, \n",
    "you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees\n",
    "\n",
    "- Train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy Score: 0.912\n",
      "Bagging Random Forest Classifier Accuracy Score: 0.912\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "\n",
    "# Write your RandomForestClassifier classifier using sklearn RandomForestClassifier class and train it. \n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate/print the prediction accuracy of the rnd_clf for Test Data\n",
    "y_pred_rnd = rnd_clf.predict(X_test)\n",
    "accuracy_rnd = accuracy_score(y_test, y_pred_rnd)\n",
    "print(\"Random Forest Classifier Accuracy Score:\", accuracy_rnd)\n",
    "\n",
    "# Write your RandomForestClassifier classifier using BaggingClassifier equivalent and train it.(estimator=500, leafnode=16)\n",
    "bag_rnd_clf = BaggingClassifier(DecisionTreeClassifier(max_leaf_nodes=16), n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate/print the prediction accuracy of the bag_rnd_clf for Test Data\n",
    "y_pred_bag_rnd = bag_rnd_clf.predict(X_test)\n",
    "accuracy_bag_rnd = accuracy_score(y_test, y_pred_bag_rnd)\n",
    "print(\"Bagging Random Forest Classifier Accuracy Score:\", accuracy_bag_rnd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2.2.5 Write Your Novel Ensemble Model \n",
    "- Write your customized Ensemble Classifier written by yourself \n",
    "- Try to get the highest score for the same dataset\n",
    "- There no limit. There is no specific constraints. Note that the classifier you write should be an Ensemble Classifier. \n",
    "\n",
    "Note: Students with the highest score on the assignment will be awarded an additional 10 points as an assignment score. All submitted scores will be ranked in descending order and the top 5 students will be awarded an additional +10 points for WH2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Ensemble Classifier Accuracy Score: 0.912\n"
     ]
    }
   ],
   "source": [
    "# Define your classifiers\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "svm_clf = SVC(probability=True, kernel='linear', C=0.5, random_state=42)\n",
    "\n",
    "# Write your Ensemble Classifier and train it.\n",
    "\n",
    "my_ensemble_clf = VotingClassifier(estimators=[('svm', knn_clf), ('rf', rf_clf),('knn', knn_clf)], voting='soft')\n",
    "\n",
    "# Train your custom ensemble classifier\n",
    "my_ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_ensemble = my_ensemble_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(\"Custom Ensemble Classifier Accuracy Score:\", accuracy_ensemble)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
